{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896c5ec5",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66cc8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbb518fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f6ffeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dde4aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29b75043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "from lightning_fabric.utilities.seed import seed_everything\n",
    "## Set the seed so that, hopefully, everyone will get the same results as me.\n",
    "#from pytorch_lightning.utilities.seed import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63649d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()\n",
    "class LightningLSTM(L.LightningModule):\n",
    "    \n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the Weights and Biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        ## input_size = number of features (or variables) in the data. In our example\n",
    "        ##              we only have a single feature (value)\n",
    "        ## hidden_size = this determines the dimension of the output\n",
    "        ##               in other words, if we set hidden_size=1, then we have 1 output node\n",
    "        ##               if we set hiddeen_size=50, then we hve 50 output nodes (that can then be 50 input\n",
    "        ##               nodes to a subsequent fully connected neural network.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1) \n",
    "         \n",
    "\n",
    "    def forward(self, input):\n",
    "        ## transpose the input vector\n",
    "        input_trans = input.view(len(input), 1)\n",
    "        \n",
    "        lstm_out, temp = self.lstm(input_trans)\n",
    "        \n",
    "        ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "        prediction = lstm_out[-1] \n",
    "        return prediction\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1) ## we'll just go ahead and set the learning rate to 0.1\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss and the predicted values so we can evaluate the training\n",
    "        ##\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42300a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[ 0.7645],\n",
      "        [ 0.8300],\n",
      "        [-0.2343],\n",
      "        [ 0.9186]])\n",
      "lstm.weight_hh_l0 tensor([[-0.2191],\n",
      "        [ 0.2018],\n",
      "        [-0.4869],\n",
      "        [ 0.5873]])\n",
      "lstm.bias_ih_l0 tensor([ 0.8815, -0.7336,  0.8692,  0.1872])\n",
      "lstm.bias_hh_l0 tensor([ 0.7388,  0.1354,  0.4822, -0.1412])\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([0.6675])\n",
      "Company B: Observed = 1, Predicted = tensor([0.6665])\n"
     ]
    }
   ],
   "source": [
    "model = LightningLSTM() # First, make model from the class\n",
    "\n",
    "## print out the name and value for each parameter\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "    \n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0f1cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('ibm.csv', sep=\";\")\n",
    "\n",
    "# Convert the DataFrame to a flat list\n",
    "flat_list = df['ibm'].values.flatten().tolist()\n",
    "#print(flat_list)\n",
    "\n",
    "scaled_list = [(x - min(flat_list)) / (max(flat_list) - min(flat_list)) for x in flat_list]\n",
    "\n",
    "#print(scaled_list)\n",
    "splitrange = round(len(scaled_list)/2)\n",
    "train_scaled_list = scaled_list[0:splitrange]\n",
    "test_scaled_list = scaled_list[splitrange+1:]\n",
    "scaled_list = train_scaled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66cfe543",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scaled_list = []\n",
    "result_scaled_list = []\n",
    "\n",
    "#Cria blocos de input com range de 5 dias\n",
    "blocos = 20\n",
    "for i in range(len(scaled_list)):\n",
    "    if i >= blocos:\n",
    "        result_scaled_list.append(scaled_list[i])\n",
    "        input_scaled_list.append(scaled_list[i-blocos:i])\n",
    "\n",
    "#print(input_scaled_list)\n",
    "#print(result_scaled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e5eb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor(input_scaled_list)\n",
    "labels = torch.tensor(result_scaled_list)\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9c0c217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | lstm | LSTM | 16    \n",
      "------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342a3c0f5c5d4a7888891da778f05184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training duration: 1656.76 seconds\n",
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[17.6984],\n",
      "        [-7.2458],\n",
      "        [10.7526],\n",
      "        [ 8.9974]])\n",
      "lstm.weight_hh_l0 tensor([[-16.8688],\n",
      "        [ -3.0060],\n",
      "        [ -7.5004],\n",
      "        [ -1.7574]])\n",
      "lstm.bias_ih_l0 tensor([ 6.2326, -3.0391,  6.1287,  0.4478])\n",
      "lstm.bias_hh_l0 tensor([ 6.0899, -2.1700,  5.7417,  0.1195])\n"
     ]
    }
   ],
   "source": [
    "## NOTE: Because we have set Adam's learning rate to 0.1, we will train much, much faster.\n",
    "## Before, with the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train\n",
    "## the model. Now, with the learning rate set to 0.1, we only need 300 epochs. Now, because we are doing so few epochs,\n",
    "## we have to tell the trainer add stuff to the log files every 2 steps (or epoch, since we have to rows of training data)\n",
    "## because the default, updating the log files every 50 steps, will result in a terrible looking graphs. So\n",
    "import time\n",
    "trainer = L.Trainer(max_epochs=30, log_every_n_steps=2)\n",
    "\n",
    "start_time = time.monotonic()\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "end_time = time.monotonic()\n",
    "\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f'Training duration: {duration:.2f} seconds')\n",
    "\n",
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4222a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scaled_list_for_test = []\n",
    "result_scaled_list_for_test = []\n",
    "\n",
    "#Cria blocos de teste com range de 60 dias\n",
    "blocos = 60\n",
    "for i in range(len(test_scaled_list)):\n",
    "    if i >= blocos:\n",
    "        input_scaled_list_for_test.append(scaled_list[i-blocos:i])\n",
    "        result_scaled_list_for_test.append(scaled_list[i])\n",
    "\n",
    "#print(input_scaled_list_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02638c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "#print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0.219034455415012, 0.24204144341591535, 0.35408746962902576, 0.23655432389056874, 0.23655432389056874])).detach())\n",
    "\n",
    "\n",
    "predicted_scaled = []\n",
    "#for i in range(len(result_scaled_list)):\n",
    "for i in range(len(input_scaled_list_for_test)):\n",
    "    predicted_scaled.append(model(torch.tensor(input_scaled_list_for_test[i]).detach()).item())\n",
    "    #predicted.append(model(torch.tensor(input_scaled_list[i]).detach()).item())\n",
    "    #model_result = model(torch.tensor(input_scaled_list[i]).detach())\n",
    "    #print(torch.max(model_result, dim=1))\n",
    "    \n",
    "#print (predicted_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c77bba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_min = min(flat_list)\n",
    "original_max = max(flat_list)\n",
    "predicted_real_value = [(x * (original_max - original_min)) + original_min for x in predicted_scaled]\n",
    "result_real_value = [(x * (original_max - original_min)) + original_min for x in result_scaled_list_for_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9c87254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"predicted.csv\", predicted_real_value, delimiter=\",\", header = \"predicted\")\n",
    "np.savetxt(\"input_ibov.csv\", input_scaled_list_for_test, delimiter=\",\", header = \"ibov\")\n",
    "np.savetxt(\"result_real_value.csv\", result_real_value, delimiter=\",\", header = \"real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99797f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0afcc036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: -1.77\n",
      "Mean squared error: 2335.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# calculate the R-squared\n",
    "r_squared = r2_score(result_real_value, predicted_real_value)\n",
    "\n",
    "# calculate the mean squared error\n",
    "mse = mean_squared_error(result_real_value, predicted_real_value)\n",
    "\n",
    "print(f'R-squared: {r_squared:.2f}')\n",
    "print(f'Mean squared error: {mse:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
